{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0hUjddM+LDNNZIe37BbAY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thedarredondo/data-science-fundamentals/blob/main/Unit6/Unit6NotesSF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gzOuXakudTC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import arviz as az\n",
        "import pymc as pm\n",
        "import graphviz as gv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit 6: Multiple Generalized Linear Modeling\n",
        "\n",
        "Last unit we learned how to predict a probalistic process using a different probablistic process--like using temperature to predict bike rentals.\n",
        "\n",
        "This unit, we'll see what happens when we allow ourselves to use multiple predictors, instead of just one. This is more powerful, but also more challenging; we'll now have to decide what and how many predictors to use.\n",
        "\n",
        "This process of variable selection is where causal diagrams really shine. Up till now, they have been a handy way to visualize how a model fits together.From here on out, causal diagrams will be a vital, dynamic part of the modeling process. They explain the concepts of Under/Overfitting, and help make our models moe interpretable.\n",
        "\n",
        "We'll also play with categorical variables, heirarchies, and introduce interactions. These are all ways to discover the relationship between different predictors.\n",
        "\n",
        "Here's what we'll cover, and the order we'll cover it in:\n",
        "1. Multiple Linear Regression with pymc and model comparison\n",
        "2. Regression with Bambi\n",
        "3. Variable Selection\n",
        "  - Causal Diagrams\n",
        "  - elpd_loo\n",
        "5. Distributional models (varable variance)\n",
        "6. Categorical variables versus Hierarchies\n",
        "7. Interactions\n",
        "\n",
        "Let's dive in."
      ],
      "metadata": {
        "id": "a0DNiuD2FCmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll spend most of this unit with the bikes data set, so that we can focus on the new concepts. I'll then throw in examples with other data sets once we've introduced all the new things"
      ],
      "metadata": {
        "id": "qnZs4DCRxB7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bikes = pd.read_csv('https://raw.githubusercontent.com/thedarredondo/data-science-fundamentals/main/Data/bikes.csv')"
      ],
      "metadata": {
        "id": "K-esvwemxW9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bikes"
      ],
      "metadata": {
        "id": "T5hzuXqjxh0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Linear Modeling with PyMC + Model Comparison with LOO\n",
        "\n",
        "We will only use base PyMC for multiple linear modeling briefly. This is because multiple linear regression requires more plotting than simple linear regression, and bambi has some nice features for quickly and easily generating those plots.\n",
        "\n",
        "We will use this oppurtunity to inroduce a new way to compare the performance of models: elpd_loo. This new method gives us a nice one number summary of which model is better, which is a useful tool to have when our models get two multidimensional to easily parse."
      ],
      "metadata": {
        "id": "pM6ogLmVyOZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You've already seen the bikes model with temperature predicting rented. The model below is almost exactly the same."
      ],
      "metadata": {
        "id": "8fhn5Xy20zb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nbb stands for negative binomial bike model\n",
        "with pm.Model() as model_nbb:\n",
        "\n",
        "    #priors for the linear part of the model\n",
        "    α = pm.Normal(\"α\", mu=0, sigma=1)\n",
        "    β = pm.Normal(\"β\", mu=0, sigma=10)\n",
        "\n",
        "    #the linear part of our model,\n",
        "    #but with a twist:\n",
        "    #our line is exponentiated, in order to make our all our values positive\n",
        "    μ = pm.Deterministic(\"μ\", pm.math.exp(α + β * bikes.temperature))\n",
        "\n",
        "    #prior for the likelihood's standard deviation\n",
        "    σ = pm.HalfNormal(\"σ\", 10)\n",
        "\n",
        "    #likelihood\n",
        "    y_pred = pm.NegativeBinomial(\"y\", mu=μ, alpha=σ, observed=bikes.rented)\n",
        "\n",
        "    #we need the log likelihood for model comparison later\n",
        "    idata_nbb = pm.sample(idata_kwargs={\"log_likelihood\":True})"
      ],
      "metadata": {
        "id": "fnt-n5kn2pOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pm.sample_posterior_predictive(idata_nbb, model = model_nbb, extend_inferencedata=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ISLHWPf_4Ah7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model below is almost exactly the same."
      ],
      "metadata": {
        "id": "HWU6Fn6n2qwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mlb stands for multiple linear bikes\n",
        "with pm.Model() as model_mlb:\n",
        "\n",
        "    #priors on the linear part of the model\n",
        "    α = pm.Normal(\"α\", mu=0, sigma=1)\n",
        "    β0 = pm.Normal(\"β0\", mu=0, sigma=10)\n",
        "    β1 = pm.Normal(\"β1\", mu=0, sigma=10)\n",
        "\n",
        "    #linear part of the model\n",
        "    μ = pm.Deterministic(\"μ\", pm.math.exp(α + β0 * bikes.temperature + β1 * bikes.hour))\n",
        "\n",
        "    #prior for the standard deviation\n",
        "    σ = pm.HalfNormal(\"σ\", 10)\n",
        "\n",
        "    #likelihood\n",
        "    y = pm.NegativeBinomial(\"y\", mu=μ, alpha=σ, observed=bikes.rented)\n",
        "\n",
        "    #we need the log likelihood for model comparison later\n",
        "    idata_mlb = pm.sample(idata_kwargs={\"log_likelihood\":True})"
      ],
      "metadata": {
        "id": "UiqXvxg51NBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pm.sample_posterior_predictive(idata_mlb, model = model_mlb, extend_inferencedata=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "atFs1wjk3JbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task1**:\n",
        "\n",
        "Looking only at the code, what's the difference between model\\_nbb and model\\_mlb?"
      ],
      "metadata": {
        "id": "5HePkNrS4FzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difference is much more clear when I make a causal diagram representing each model"
      ],
      "metadata": {
        "id": "SKfHuKvuL0NU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#single predictor causal diagrm\n",
        "dag_b = gv.Digraph(comment='bikes_dag') #b for bikes\n",
        "\n",
        "dag_b.node('R', 'rented bikes')\n",
        "dag_b.node('T','temperature')\n",
        "\n",
        "dag_b.edges(['TR',])\n",
        "\n",
        "dag_b"
      ],
      "metadata": {
        "id": "AIcALxSnL5Nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#multiple predictor causal diagram\n",
        "dag_b = gv.Digraph(comment='bikes_dag') #b for bikes\n",
        "\n",
        "dag_b.node('R', 'rented bikes')\n",
        "dag_b.node('T','temperature')\n",
        "dag_b.node('H','hour')\n",
        "\n",
        "dag_b.edges(['TR', 'HR',])\n",
        "\n",
        "dag_b"
      ],
      "metadata": {
        "id": "jeN_O0XMMIEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the two models' preformance using a ppc"
      ],
      "metadata": {
        "id": "SVJqJwqj46By"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_ppc(idata_nbb, num_pp_samples=200, colors=[\"C1\", \"C0\", \"C1\"])"
      ],
      "metadata": {
        "id": "quLNaOY_4wYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_ppc(idata_mlb, num_pp_samples=200, colors=[\"C1\", \"C0\", \"C1\"])"
      ],
      "metadata": {
        "id": "BXJR-tMQ3Dff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task2**:\n",
        "\n",
        "Describe any differences you can see."
      ],
      "metadata": {
        "id": "Isr4ha2_AVyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully you're wondering if there is another way to compare these two models.\n",
        "\n",
        "And there is!\n",
        "\n",
        "The method is called Pareto Smoothed Importance Sampling Leave-One-Out Cross-Validation, which estimates the Expected Log-Pointwise-predictive Density.\n",
        "\n",
        "We abbreviate all that with ELPD$_{LOO-CV}$, or elpd_loo, or even just LOO.\n",
        "\n",
        "As the long full name implies, there are a lot of advanced, fancy tricks applied to the posterior to calculate LOO.\n",
        "\n",
        "The important things to know:\n",
        "- a more positive elpd_loo is better, but only in comparison to another model.\n",
        "- the standard error (SE), sorta like the standard deviation, of elpd_loo helps tell us whether two models are significantly different than one another in terms of performance.\n",
        "- There's something called a Pareto k diagnostic that tells us whether there were too many influential points for elpd_loo to be effective.\n",
        "\n"
      ],
      "metadata": {
        "id": "-XDAMI6qBo9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the k diagnostic for the multiple linear model."
      ],
      "metadata": {
        "id": "qphICmvyRLvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az.loo(idata_mlb)"
      ],
      "metadata": {
        "id": "tU0-ox3D83e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All 348 values in the bikes data set have a small enough influence that we can almost completely trust elpd_loo's recommendation.\n",
        "\n",
        "We also get the value of elpd_loo (-2124.37), and its standard error (23.36). A more positive elpd_loo is better--But these are still useless without another model to compare them too.\n",
        "\n",
        "Why? Because maybe the process that generated the data is intrisically hard to predict. In other words, maybe even the best model does pretty bad (a great example of this is the stock market). We won't know if that's the scenario we're in until we be *at least* one more model."
      ],
      "metadata": {
        "id": "IHWUQu0jF-5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do have another model to compare though! And arviz has some nice functions that will make the comparison trivial."
      ],
      "metadata": {
        "id": "n-oK0_7lSZOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the table version, which I think is less useful that the visualization below it.\n",
        "\n",
        "The table does indclude info about a  warning though which is useful. As long as it says False, we're good--False means our k diagnostic was good enough. The other interesting entry is weight. This can be used to average the models being compared, using the following code:\n",
        "\n",
        "```\n",
        "idata_w = az.weight_predictions([idata_mlb,idata_nbb], weights=[0.903435, 0.096565])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "2xGqDXxNG5Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cmp_df stands for compare dataframe\n",
        "cmp_df = az.compare( {\"multi_lin\":idata_mlb,\"single_lin\":idata_nbb} )\n",
        "\n",
        "#0 is the best rank; we want a lower elpd_loo\n",
        "cmp_df"
      ],
      "metadata": {
        "id": "nPgXmGrn75ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "elpd_loo and se are easier to understand using the graph below."
      ],
      "metadata": {
        "id": "batZV0KRGI2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_compare(cmp_df)"
      ],
      "metadata": {
        "id": "a0DVdFiQ-cLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say we don't want to combine the models though, but choose between them. Then its often easier to graph the above table, then make a decision."
      ],
      "metadata": {
        "id": "wK8rxGl7VUpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task3**:\n",
        "\n",
        "Using the model comparion plot, which model would you pick? Why?"
      ],
      "metadata": {
        "id": "krOoMG61VilG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE**: elpd_loo is *NOT* the definitive say on which model is better. You MUST take conext into account. This is done formally by referencing the causal diagram, and deciding which casual diagram is a better map to reality.\n",
        "\n",
        "Review the causal diagrams for each model. Which seems more realistic?\n",
        "\n",
        "One final comment for this section: elpd_loo is basically a summary of the posterior predictive check. If one of your models has a clearly better posterior predicitve check based on visual, you do not need to bother with elpd_loo. We calculate elpd_loo when we feel like our visual inspection isn't enough."
      ],
      "metadata": {
        "id": "_nPZAL33NJDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bambi is Best\n",
        "For (generalized) linear models, anyway.\n",
        "\n",
        "Bambi is library for building bayesian (generalized) linear models.\n",
        "\n",
        "Here are the two main two reasons to love bambi:\n",
        "\n",
        "- by auto assigning normal and half normal priors with mean 0 and sd 1, bambi models are fast to write and read.\n",
        "- it has built in methods for plotting the posterior means and posterior predictive distribution. No more giant blocks of code to create plots for linear models!\n",
        "\n",
        "Here are the two main reasons to be cautious with bambi:\n",
        "\n",
        "- it only works with (generalized) linear models. As in, using bambi means that we are assuming some type of linear model.\n",
        "- bambi auto assigns normal and half normal priors with a mean of 0 and sd of 1. This isn't that big a deal, since there's a quick way to feed the priors we want into bambi. Its easy to get lazy with bambi, which is fine, until it isn't."
      ],
      "metadata": {
        "id": "uhVIfB99X2Xk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab also doesn't have bambi preinstalled, so we need to install it each time we pull up colab.\n",
        "\n",
        "We will also need to install the latest version of xarray, since bambi relies on its latest features.\n",
        "\n",
        "Note that you may get prompted to restart the session, especially if you've already run pymc and/or arviz. Go ahead and do so, if prompted."
      ],
      "metadata": {
        "id": "eyB7BF6ynz9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bambi"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FB8Wdy8GYTL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bambi as bmb"
      ],
      "metadata": {
        "id": "PG0tF4j_YP8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import arviz as az\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "E2JUdnuwo-xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Bikes + Regularizing Priors\n",
        "\n",
        "Let's start by remaking the two bike models from earlier in the unit, but with bambi.\n",
        "\n",
        "We'll also discuss the concept of regularizing priors; specifically, we'll see the the weakly informative priors bambi defaults too oftenn work great in practice."
      ],
      "metadata": {
        "id": "cOpC2SIKd0sY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bambi bikes (1 predictor)\n",
        "We'll start with the single variable model."
      ],
      "metadata": {
        "id": "Nd-NUqqdeBkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we put the priors that aren't normal or half normal with mean 0 and sigma 1\n",
        "#in a dict\n",
        "priors ={\"temperature\" : bmb.Prior(\"Normal\", mu=0,sigma=10), \"alpha\" : bmb.Prior(\"HalfNormal\",sigma=10)}\n",
        "\n",
        "#This creates a skeleton of the model; it hasn't run MCMC or created a posterior yet\n",
        "model_nbb_bmb = bmb.Model(\"rented ~ temperature\", bikes, family = \"negativebinomial\", priors=priors)"
      ],
      "metadata": {
        "id": "qaIBqB6zfQBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_nbb_bmb"
      ],
      "metadata": {
        "id": "JTVKZewGvKrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'family' argument in bmb.Model() is what determines both our likelihood and our link function. Bambi has default link functions that work well with families of likelihoods. Note the mu = log(x) is the same as exp(mu)=x.\n",
        "\n",
        "Specifying 'family' is optional; the default is a normal likelihood.\n",
        "\n",
        "Also notice that defining the model only takes two lines of code, and we only need one more to make the model it self."
      ],
      "metadata": {
        "id": "fTRpufTyqlZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is bambi's equivalent of pm.sample()\n",
        "idata_nbb_bmb = model_nbb_bmb.fit(idata_kwargs={\"log_likelihood\":True})"
      ],
      "metadata": {
        "id": "N_-5Q8_8gJ2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two more lines gives us a plot of the posterior mean."
      ],
      "metadata": {
        "id": "Y8iN6U_BrLXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bmb.interpret.plot_predictions(model_nbb_bmb, idata_nbb_bmb, \"temperature\")\n",
        "plt.plot(bikes.temperature, bikes.rented, \"C2.\", zorder=-3)"
      ],
      "metadata": {
        "id": "HX0oQLKnlsPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A final two lines give us the posterior predictive distribution.\n",
        "\n",
        "**This is the visualization we will use to assess our models performance. It is bascially a posterior predictive check, and thus can be used to compare models.**\n",
        "\n",
        "In both graphs, the shaded blue area is a 94% HDI. You can change this by using the prob argument in the plot_predictions method."
      ],
      "metadata": {
        "id": "s4WQu5I6QGY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#note the 'pps=True' argument added in to get a full posterior predictive check\n",
        "bmb.interpret.plot_predictions(model_nbb_bmb, idata_nbb_bmb, \"temperature\", pps=True)\n",
        "plt.plot(bikes.temperature, bikes.rented, \"C2.\", zorder=-3)"
      ],
      "metadata": {
        "id": "yTb1AppTmRyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want the one dimentional posterior predictive check from before, you can get it with the following code:"
      ],
      "metadata": {
        "id": "PK9Mfsi_wiLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_nbb_bmb.predict(idata_nbb_bmb, kind=\"response\")\n",
        "az.plot_ppc(idata_nbb_bmb)"
      ],
      "metadata": {
        "id": "U8WkSW9jwwMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Regularizing Priors (weakly informative priors)\n",
        "\n",
        "We do not have to specify priors get a bambi model to run; bambi has a routine to determine the priors based off the raw data. Remember, that's the same strategy I told you to employ when you needed to specify prior(s) for which you had no context.\n",
        "\n",
        "So, if you're handed data you know next to nothing about, you can safely selected a likelihood, and fire the model up.\n",
        "\n",
        "But if you do have an understanding of the context, your final model should reflect your knowledge in its priors."
      ],
      "metadata": {
        "id": "LvgOggV2tlWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#notice that the priors argument isn't specified\n",
        "model_nbb_bmb_regpriors = bmb.Model(\"rented ~ temperature\", bikes, family = \"negativebinomial\")\n",
        "idata_nbb_bmb_regpriors = model_nbb_bmb_regpriors.fit(idata_kwargs={\"log_likelihood\":True})"
      ],
      "metadata": {
        "id": "arKOMn3iw_GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you do let bambi select priors for you, then you must print out the model, to see what it selected. That way, you can check if its selections jive with your inuition."
      ],
      "metadata": {
        "id": "0LdNfY2vxRc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_nbb_bmb_regpriors"
      ],
      "metadata": {
        "id": "j2Vu3qjSxNff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the posterior mean and posterior predictive."
      ],
      "metadata": {
        "id": "-G17eOHhxig9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bmb.interpret.plot_predictions(model_nbb_bmb_regpriors, idata_nbb_bmb_regpriors, \"temperature\")\n",
        "plt.plot(bikes.temperature, bikes.rented, \"C2.\", zorder=-3)"
      ],
      "metadata": {
        "id": "wmZuW2s5xiI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bmb.interpret.plot_predictions(model_nbb_bmb_regpriors, idata_nbb_bmb_regpriors, \"temperature\", pps = True)\n",
        "plt.plot(bikes.temperature, bikes.rented, \"C2.\", zorder=-3)"
      ],
      "metadata": {
        "id": "7cdc0FKjxxuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task4**:\n",
        "\n",
        "Are their any significant differences in the graphs of model_nbb_bmb and model_nbb_regpriors?\n",
        "\n",
        "Why or why not?"
      ],
      "metadata": {
        "id": "y2DRy8hXyZwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bambi bikes (multiple predictors)\n",
        "\n",
        "Now that we know what a regularings prior is, and why they're often good enough, let's recreate the model with both temperature and hour.\n",
        "\n",
        "And since I was using the data to create my priors anyway, I'll let bambi choose my priors."
      ],
      "metadata": {
        "id": "BA1UAaIBsHuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_mlb_bmb = bmb.Model(\"rented ~ temperature + hour\", bikes, family=\"negativebinomial\")\n",
        "idata_mlb_bmb = model_mlb_bmb.fit(idata_kwargs={\"log_likelihood\":True})"
      ],
      "metadata": {
        "id": "F7LJfwmZuJA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I need to print the model, so I can see what priors bambi went with."
      ],
      "metadata": {
        "id": "kvQLUeQe0FeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_mlb_bmb"
      ],
      "metadata": {
        "id": "5UTVt-oAuZO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the model, so we can actually see what it suggests."
      ],
      "metadata": {
        "id": "zxaUXGyJ0QVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bmb.interpret.plot_predictions(model_mlb_bmb, idata_mlb_bmb, [\"temperature\", \"hour\"],\n",
        "                               subplot_kwargs={\"group\":None, \"panel\":\"hour\"},\n",
        "                               legend=False,\n",
        "                               fig_kwargs={\"sharey\":True, \"sharex\":True})"
      ],
      "metadata": {
        "id": "9jQKJSTE0X3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task5**:\n",
        "\n",
        "Describe the relationship between the slope of temperature and the hour of the day, in model_mlb_bmb."
      ],
      "metadata": {
        "id": "7_ObTF7A5dWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bambi models are equally compatible with arivz, since they are running pymc under the hood. This means everything we learned about elpd_loo still applies."
      ],
      "metadata": {
        "id": "ohf3EI35-2j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cmp_df_bmb = az.compare( {\"multi_lin\":idata_mlb_bmb,\"single_lin\":idata_nbb_bmb} )\n",
        "cmp_df_bmb"
      ],
      "metadata": {
        "id": "P8aSM7_m-BS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_compare(cmp_df_bmb)"
      ],
      "metadata": {
        "id": "tUZCAb-X-BPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variable Selection\n",
        "\n",
        "When using multiple predictors, some logical questions arise:\n",
        "- do we really need multiple predictors?\n",
        "- if so, how many do we need?\n",
        "- and which ones? Are some better than others? The same as others?\n",
        "\n",
        "The process of answering those questions is known as variable selection."
      ],
      "metadata": {
        "id": "H6trQtjl_One"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task6**:\n",
        "\n",
        "Given what you know and what we've learned thus far, how would you go about selecting which variables are best?"
      ],
      "metadata": {
        "id": "CG148LXNA8n2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regardless of what you wrote as an answer, make sure to read the answer key before moving on to the next task."
      ],
      "metadata": {
        "id": "1wl0msq_y60R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task7:**\n",
        "\n",
        "Make a causal diagram for the bikes data context that is, in your opinion:\n",
        "- the most performative\n",
        "- the most realistic\n",
        "\n",
        "Then, justify why it is realistic with typed text.\n",
        "\n",
        "Finally, justify why it is performative using elpd_loo. Note that this requires reference to *at least* one other model."
      ],
      "metadata": {
        "id": "5pj1H9-nKWPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, regardless of what you did, make sure to **read the answer key before moving on**."
      ],
      "metadata": {
        "id": "szup1w7el_ZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pair plots/Scatter plot Matricies\n",
        "\n",
        "Before I mention how to make an interpret pair plots/scatter plot matricies, know that the comibnation of casual diagrams and the posterior predictive distributions of bayesian models (or their summaries using elpd) is the best way to \"analyze data\" as of this writing.\n",
        "\n",
        "In order to help us figure out what bayesian model to contruct, and what casual diagram to draw, it often helps to plot each variables agaisnt one another.\n",
        "\n",
        "We do this using a scatter plot matrix, or pair plot (a plot for each pair of variables)."
      ],
      "metadata": {
        "id": "6Jukp9yMimWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(bikes)"
      ],
      "metadata": {
        "id": "-Ia8R57_s3qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is literally all possible scatter plots you can make with your dataframe."
      ],
      "metadata": {
        "id": "NmlHJcR1nziX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**:\n",
        "\n",
        "How could we have used this scatter plot matrix/pair plot to help us draw our casual diagram--or even to reason out that hour is the only variable of interest, at least in this dataset?"
      ],
      "metadata": {
        "id": "ci5YDDemn8dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: Pairplots are NOT objective. Objectivity isn't real.\n",
        "\n",
        "Even if there do not seem to be strong trends in the pair plot, run a quick model anyway, and get a visual of a posterior predictive.\n",
        "\n",
        "Pairplots only look at pairs of variables, or in 2 dimensions; something interesting might happen in higher dimensions (more variables at once). Usually, the pairplot is a good indicator of when there are patterns in the data--but \"usually\" is not all the time, and the exceptions are almost always interesting. We'll see an \"example\" of such a thing in Unit 8."
      ],
      "metadata": {
        "id": "homTA3EYofo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Things to Know about Variable Selection:\n",
        "\n",
        "Our defaults:\n",
        "1. Make a casual diagram, and a model based on that causal diagram. Use a pair plot to help guide you--but don't let it dicate all your decisions.\n",
        "\n",
        "2. Make a \"reference model\" with as many available variables as possible in it.\n",
        "\n",
        "3. Compare the posterior predictve distributions of each, and the elpd_loo while you're at it. This is the *minimum* amount of steps you should do.\n",
        "    - You only stop here if the posterior predictive of your causal diagram inspired model looks spectacular--and even then, you should consider doing the next steps.\n",
        "\n",
        "4. Start making models with less variables than the causal diagram inspired model and/or reference model--whichever was more performative in the previous step.\n",
        "\n",
        "5. Compare the new models to your previous models. If you have any that do better with less variables, then make new ones with even less variables. See how low you can go without sacrificing performance.\n",
        "\n",
        "6. Pick the best performing model with the lowest number of variables. Make a causal diagram for the model, if you have not already done so.\n",
        "\n",
        "7. Compare that casual diagram to your originial diagram, and decide which one is more realistic. The models' performacne--posterior predictive visualization, elpd_loo--should influence your decision.\n",
        "\n",
        "8. You now have your final model, the one that best predicts reality and best explains the causal relationships in reality, at least accoring to your analysis, and your data.\n",
        "\n",
        "Why do we have a bias towards models with fewer variables? For now, we want to be more wary of overfitting as opposed to underfitting.\n",
        "\n",
        "Overfitting is when our model seems to predict the data we have on hand well--but those predictions don't generalize to new, similar data. In other words, our model over fits the curve of the data, and doesn't allow enough room to adapt to the inherent variation in reality.\n",
        "\n",
        "Underfitting is when we could have made better predictions, but our model is not complex enough to make the right predictions. Its easy to underfit with a GLM, but in the next three units we will learn new techniques which will make underfitting unlikely, to put it generously.\n",
        "\n",
        "I want y'all to get in the habit of slowing your enthusiasm to make the curve fit as best as possible. We don't want to fit the curve, we want to make predictions that will help improve our decision making. Fitting the curve is a means to an end, a tool that can break, just like any other tool.\n",
        "\n"
      ],
      "metadata": {
        "id": "uqB6TvjH2l4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### More Nuance on Bambi, Overfitting, and Regularizing Priors\n",
        "\n",
        "Technically, the regulizing priors that bambi selected based on the data go a long way to making sure our predictions won't be too bad on new data, even if we include an extra few variables.\n",
        "\n",
        "I'll say that again: weakly informative, or regularizing, priors do a lot to prevent overfitting. Priors are pretty much *the* way to help prevent overfitting.\n",
        "\n",
        "In fact, having a bias towards models with less variables can itself be constructed as a prior. We're not going to make our preference that explicit, but the point is that we could.\n",
        "\n",
        "The reason we won't make it that explicit, is that we want variable selection to be fluid. If we think an extra variable or two could capture information that happened to not be in this data set, then by all means, use your contextual knowledge to selcect a model with more variables.\n",
        "\n",
        "This is especially true if our casual model includes lots of variables, and we our confident in our casual model.\n",
        "\n",
        "However, if we know that collecting data on more variables is expensive or dangerous, then we can select a smaller model.\n",
        "\n",
        "Variable selection is complicated. **You must draw a casual diagram**, calculate elpd_loo, compare to another model, adjust accordingly and then repeat that process until you end up with something you believe in."
      ],
      "metadata": {
        "id": "g5CVdBsdjIFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distributional models: Dealing with variable variance\n",
        "\n",
        "We'll now take a quite detour back to single predictor land, so that I can show you how to account for variable variance with bambi.\n",
        "\n",
        "When we build a linear regression model, there is at least one linear equation applied in our formulas--usually to the location/center parameter of our likelihood, which we've been calling mu.\n",
        "\n",
        "Applying a linear equation to more than one parameter of our likelihood is called a distributional model. We've done this before with the babies dataset. We'll do it again now, but use bambi.\n"
      ],
      "metadata": {
        "id": "mFNTnE_eRXo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "babies = pd.read_csv(\"https://raw.githubusercontent.com/thedarredondo/data-science-fundamentals/main/Data/babies.csv\")"
      ],
      "metadata": {
        "id": "joU3GpFTTY_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we need to specify the two regression relationships with a bambi method\n",
        "formula_babies = bmb.Formula(\n",
        "    \"length ~ np.sqrt(month)\",\n",
        "    \"sigma ~ month\"\n",
        ")\n",
        "\n",
        "#dis for distributional model\n",
        "model_dis = bmb.Model(formula_babies, babies)\n",
        "\n",
        "#I only need \"idata_kwargs={\"log_likelihood\":True}\"\" if I'm going to use\n",
        "#elpd_loo for variable selection. So I could have dropped it here\n",
        "idata_dis = model_dis.fit(idata_kwargs={\"log_likelihood\":True})"
      ],
      "metadata": {
        "id": "jopUcVr8Tna7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how to get the kruschke diagram as well. I didn't do this with the bikes models because this function breaks when the likelihood is a negative binomial."
      ],
      "metadata": {
        "id": "vKkMqocEQ9g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_dis.graph()"
      ],
      "metadata": {
        "id": "0dbXua75Q5oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shows me what priors/likelihood bambi used.\n",
        "model_dis"
      ],
      "metadata": {
        "id": "M4hxFeF2ik5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#there's more code here so I could plot two HDIs\n",
        "_, ax = plt.subplots(sharey=True, sharex=\"col\", figsize=(12, 6))\n",
        "#mean line\n",
        "bmb.interpret.plot_predictions(model_dis, idata_dis, \"month\", ax=ax, fig_kwargs={\"color\":\"k\"})\n",
        "#94% HDI\n",
        "bmb.interpret.plot_predictions(model_dis, idata_dis, \"month\", pps=True, ax=ax)\n",
        "#65% HDI\n",
        "ax_ = bmb.interpret.plot_predictions(model_dis, idata_dis, \"month\", pps=True, ax=ax, prob=0.65)\n",
        "ax_[1][0].get_children()[5].set_facecolor('C1')\n",
        "\n",
        "#raw data\n",
        "ax.plot(babies.month, babies.length, \"C2.\", zorder=-3)"
      ],
      "metadata": {
        "id": "5fmZzN-pUn5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same thing as in the previous unit. Neat."
      ],
      "metadata": {
        "id": "Zt-0y-ixj1aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorical variables, Hierarchies, and Interactions\n",
        "\n",
        "With the basics of multiple linear regression and bambi behind us, we can now look at the various ways variables can intereact. A tad onfusingly, only one of these methods is called an interaction.\n",
        "\n",
        "We'll recreate:\n",
        "- see how categorical variables work with multiple linear regression\n",
        "- examine hierearchies through bambi\n",
        "- introduce interactions, in light of categorical variables and hierarchies.\n",
        "\n",
        "Note: all priors will be regularizing priors, or weakly informative priors, that bambi will select based on the data itself.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hIWXKrtcFfJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorical variables versus Hierarchies\n",
        "\n",
        "Let's get back into the action with something new: creating a model with categorical variables and quantitative variables. Sepcifically, one categorical and several quantitative variables.\n",
        "\n",
        "We'll also create a hierarchical regression with that same categorical variable, and discuss when to treat it as its own variable, and when to think of it as a hierarchy."
      ],
      "metadata": {
        "id": "NofZjz_vj4TG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#have to drop incomplete rows, so that bambi will run\n",
        "basketball = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/thedarredondo/data-science-fundamentals/main/Data/basketball2324.csv').dropna()"
      ],
      "metadata": {
        "id": "kfVxvltJlOa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#only look at players who played more than 400 minutes\n",
        "basketball = basketball.query('MP > 400')\n",
        "#remove players who never missed a free throw\n",
        "basketball = basketball.query('`FT%` != 1.0')"
      ],
      "metadata": {
        "id": "oevLDINKridn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filter out the combo positions. This while make it easier to read the graphs\n",
        "basketball = basketball.query(\"Pos in ['C','PF','SF','SG','PG']\")"
      ],
      "metadata": {
        "id": "2Rb6cpo3oa0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the model\n",
        "model_basketball = bmb.Model(\"`FG%` ~ `FT%` + Pos\", data=basketball)\n",
        "#fit the model\n",
        "idata_basketball = model_basketball.fit(idata_kwargs={'log_likelihood': True})"
      ],
      "metadata": {
        "id": "mW9I54himlhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_basketball.graph()"
      ],
      "metadata": {
        "id": "L3hmW6mcRoaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make that last diagram a little more readable, here's the causal diagram:"
      ],
      "metadata": {
        "id": "T1X6sPeCSgbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dag_bball = gv.Digraph(comment='bball_dag') #bball b/c ball is life <3\n",
        "\n",
        "dag_bball.node('G', 'FG%')\n",
        "dag_bball.node('T','FT%')\n",
        "dag_bball.node('P','Pos')\n",
        "\n",
        "dag_bball.edges(['TG','PG',])\n",
        "\n",
        "dag_bball"
      ],
      "metadata": {
        "id": "7OxG_nF8Slb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot 94% HDIs of the means of each position\n",
        "bmb.interpret.plot_predictions(model_basketball,\n",
        "                               idata_basketball,\n",
        "                                [\"FT%\",  \"Pos\"], fig_kwargs={\"figsize\":(11, 4)})"
      ],
      "metadata": {
        "id": "0rpA5H7xn5Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before explaining what's going on, I'm goin to jump right in the using Pos as a hierarchy."
      ],
      "metadata": {
        "id": "lIIfE5hHpmoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the model, but as a hierarchy\n",
        "model_basketball_h = bmb.Model(\"`FG%` ~ (`FT%`|Pos)\", data=basketball)\n",
        "#create the model\n",
        "idata_basketball_h = model_basketball_h.fit(idata_kwargs={'log_likelihood': True})"
      ],
      "metadata": {
        "id": "XBqYP_1jp1KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_basketball_h.graph()"
      ],
      "metadata": {
        "id": "ahwCVdVKRv_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here's the causal diagram, so you can see how the variables connect"
      ],
      "metadata": {
        "id": "RwqZ7MHvTp9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dag_bballh = gv.Digraph(comment='bballh_dag') #bball hierarchy\n",
        "\n",
        "dag_bballh.node('G', 'FG%')\n",
        "dag_bballh.node('T','FT%')\n",
        "dag_bballh.node('P','Pos')\n",
        "\n",
        "dag_bballh.edges(['TG','PG','PT',])\n",
        "\n",
        "dag_bballh"
      ],
      "metadata": {
        "id": "_jIegHb4cedH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bmb.interpret.plot_predictions(model_basketball_h,\n",
        "                               idata_basketball_h,\n",
        "                               [\"FT%\",\"Pos\"],\n",
        "                               fig_kwargs={\"figsize\":(11, 4)})"
      ],
      "metadata": {
        "id": "fCVnH0hZqId3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task9**:\n",
        "\n",
        "Identify and explain any similarities or differences in the plots of model_basketball and model_basketball_h."
      ],
      "metadata": {
        "id": "gPvrC5Tepmkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interactions\n",
        "\n",
        "It's often the case that the effect of a predictor on the respone variable is affected by a different predictor variable.\n",
        "\n",
        "I'll continue to use the basketball data set for this, and I'll use one example with a categorical/quantitative variable combo, and one with two quantitative variables."
      ],
      "metadata": {
        "id": "9TKIn2_2w2eD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the model; bb_inter stands for baskeball interaction\n",
        "model_bb_inter = bmb.Model(\"`FG%` ~ `FT%` + Pos + `FT%`:Pos\", data=basketball)\n",
        "#create the model\n",
        "idata_bb_inter = model_bb_inter.fit(idata_kwargs={'log_likelihood': True})"
      ],
      "metadata": {
        "id": "6gi6LKlAx4lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_bb_inter.graph()"
      ],
      "metadata": {
        "id": "Ph1S4TUdR0wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dag_bballi = gv.Digraph(comment='bball_dag') #bball interaction\n",
        "\n",
        "dag_bballi.node('G', 'FG%')\n",
        "dag_bballi.node('T','FT%')\n",
        "dag_bballi.node('P','Pos')\n",
        "\n",
        "dag_bballi.edge('P', 'T', dir='both')\n",
        "dag_bballi.edges(['TG','PG',])\n",
        "\n",
        "dag_bballi"
      ],
      "metadata": {
        "id": "8IgFrHs1Uf_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bmb.interpret.plot_predictions(model_bb_inter,\n",
        "                               idata_bb_inter,\n",
        "                               [\"FT%\",\"Pos\"],\n",
        "                               fig_kwargs={\"figsize\":(11, 4)})"
      ],
      "metadata": {
        "id": "5GpdBjiYySNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task10**:\n",
        "\n",
        "Compare this with the previous two graphs. How has the interaction affected the slope of FT% with respect to FG%?"
      ],
      "metadata": {
        "id": "ECtRec20zfbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do an interaction effect with all quantiative variables. For this model, I'll just replace Pos with 3PA."
      ],
      "metadata": {
        "id": "gYCieHPQ1l0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the model; bb_interq stands for baskeball interaction quantiative\n",
        "model_bb_interq = bmb.Model(\"`FG%` ~ `FT%` + `3PA` + `FT%`:`3PA`\", data=basketball)\n",
        "#create the model\n",
        "idata_bb_interq = model_bb_interq.fit(idata_kwargs={'log_likelihood': True})"
      ],
      "metadata": {
        "id": "zlJ2pt3c1Tv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the causal structure is the same as the other interaction model, just with 3PA instead of Pos."
      ],
      "metadata": {
        "id": "qnFdl0sObqsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dag_bballi = gv.Digraph(comment='bball_dag') #bball interaction\n",
        "\n",
        "dag_bballi.node('G', 'FG%')\n",
        "dag_bballi.node('T','FT%')\n",
        "dag_bballi.node('A','3PA')\n",
        "\n",
        "dag_bballi.edge('A', 'T', dir='both')\n",
        "dag_bballi.edges(['TG','AG',])\n",
        "\n",
        "dag_bballi"
      ],
      "metadata": {
        "id": "thx4WSLibp9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bmb.interpret.plot_predictions(model_bb_interq,\n",
        "                               idata_bb_interq,\n",
        "                               [\"FT%\",\"3PA\"],\n",
        "                               fig_kwargs={\"figsize\":(11, 13)})#,\n",
        "                               #legend=False)"
      ],
      "metadata": {
        "id": "gMz3-AM411eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task11**:\n",
        "\n",
        "Using the plot above, describe the effect of 3PA on slope of FT% with respect to FG%."
      ],
      "metadata": {
        "id": "SlobQtlq10Ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task12**:\n",
        "\n",
        "Using the visualizations of model_bb_inter and model_bb_interq, answer the following:\n",
        "1. Do centers (C) have, on average, low 3PA compared to other positions?\n",
        "2. Is FT% a good predictor of FG%?"
      ],
      "metadata": {
        "id": "K-TofbXx4wqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "As usual, we've covered a lot of ground in one unit.\n",
        "\n",
        "We:\n",
        "\n",
        "- learned how to use multiple predictors at once, both in vanilla pymc and using a new library, bambi.\n",
        "- saw saw how causal diagrams and a new metric called elpd_loo can helps us select variables and models\n",
        "- Practiced comparing and contrasting the predictions and fit of categorical varibles, hierarchies, and interactions.\n",
        "\n",
        "And now I say, congradulations! You have almost all the basics of generalized linear models at your disposal now. This is a big deal; GLMs are often a good approximation of many processes, and they are often interpretable. There's plenty more to learn, but the paths in front of you have now become much more varied.\n",
        "\n",
        "In the next unit, we'll cover the remaining basics of glms (polynomial regression, b splines) in order to give you inuition for the most powerful modeling tool of all, a tool from which nerual networks are but one instance of: Gaussian Processes."
      ],
      "metadata": {
        "id": "l4kOhohXhLOb"
      }
    }
  ]
}
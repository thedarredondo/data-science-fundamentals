{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thedarredondo/data-science-fundamentals/blob/main/Unit8/Unit8NotesSF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YSkoMLi-xN5"
      },
      "outputs": [],
      "source": [
        "import arviz as az\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymc as pm\n",
        "import pymc_bart as pmb\n",
        "import seaborn as sns\n",
        "import graphviz as gv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymc-bart\n",
        "import pymc_bart as pmb"
      ],
      "metadata": {
        "id": "xh8BCvWb4Kei",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit 8: Bayesian Additive Regression Trees (BART)\n",
        "\n",
        "Bayesian Additive Regression Trees (BART) can be thought of as a fast approximation of Gaussian Processes (GPs). The specific way BART models work has various limitations, quirks, and benefits; we'll dicuss them all in this unit.\n",
        "\n",
        "We'll learn:\n",
        "- what a decision tree is\n",
        "- how BART models work, and their relationshipp to decision trees\n",
        "- how to implement BART in PyMC\n",
        "- Partial Dependence Plots (pdp)\n",
        "- Individual Conditional Expecation plots (ice)\n",
        "- variable importance (vi) plots\n",
        "\n",
        "Let's get started."
      ],
      "metadata": {
        "id": "j1DVpKSi_COG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Theoretical Background: Trees and Forests\n",
        "\n"
      ],
      "metadata": {
        "id": "HA8uPRyqB3ly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Trees\n",
        "\n",
        "Think of decision trees as flow charts (the technical term is a graph), but with several restrictions.\n",
        "- Each node can have only one 'parent' node, but many children.\n",
        "- We'll focus on binary decision trees, where each node has one parent, and two or zero children\n",
        "- There is a special node called the 'root' node, with no parents\n",
        "- Each layer of the tree sorts the predictor values into subsets of the predicted values\n",
        "\n",
        "All thos points are best illustrated with an example.\n",
        "\n",
        "For our example, we'll use some data on octupus beaks, and try to use upper beak length (estimator) to predict total beak weight (estimand).\n"
      ],
      "metadata": {
        "id": "PNf6ytgaFRQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree"
      ],
      "metadata": {
        "id": "_RmFmcxrUjaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make sure to read my comments!**"
      ],
      "metadata": {
        "id": "IEQzQkeOdCJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data on octupus beaks\n",
        "octps = pd.read_csv('https://raw.githubusercontent.com/thedarredondo/data-science-fundamentals/refs/heads/main/Data/octopusbeakweight_nlin.csv')"
      ],
      "metadata": {
        "id": "nVXoH7LhVN7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X is our predictor variable, upper beak measurment\n",
        "X = octps[\"upBeak\"].to_numpy().reshape(-1, 1)\n",
        "#Y is the predicted variable, total beak weight\n",
        "Y = octps[\"totWt\"].to_numpy()"
      ],
      "metadata": {
        "id": "qMUMSHUdVWHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fits a decision tree. This is analogous to running pm.sample,\n",
        "#except it only finds the mean . . . kinda like that, anyway.\n",
        "dtree = tree.DecisionTreeRegressor(max_depth=2)\n",
        "octtree = dtree.fit(X,Y)\n",
        "\n",
        "#uses the fitted model to predict total weight for various\n",
        "#unseen upper beak measurements\n",
        "#This is analogous to a posterior predictive distribution . . .\n",
        "#. . . if we only found the posterior predictive mean...\n",
        "#...kinda\n",
        "X_test = np.arange(75, 250, 1)[:, np.newaxis]\n",
        "y = octtree.predict(X_test)"
      ],
      "metadata": {
        "id": "OfU5LwG1Uk1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the graph (flow chart-y) version of the decision tree."
      ],
      "metadata": {
        "id": "0G98-S7KdnAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree.plot_tree(octtree)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wRmlZlS5WI66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below graph is similar to if I'd plot just the posterior predictive mean."
      ],
      "metadata": {
        "id": "fmAm7NloduPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.scatter(X, Y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"actual measurements\")\n",
        "plt.plot(X_test, y, color=\"cornflowerblue\", linewidth=2)\n",
        "plt.xlabel(\"upBeak\")\n",
        "plt.ylabel(\"totWt\")\n",
        "plt.title(\"Decision Tree Regression\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PL1gk26uWsnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task1**:\n",
        "\n",
        "The graph (flow charty) version of the tree has four nodes with no children, often called 'leaf' nodes.\n",
        "\n",
        "Each of those nodes something that says \"value =\", then a number.\n",
        "\n",
        "What are those numbers, and what do they have to do with the psuedo-posterior predictive mean from the scatter plot?"
      ],
      "metadata": {
        "id": "X2hx8Bi8dNYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task2**:\n",
        "\n",
        "There are three other nodes in the graph (flow chart-y) version of the decision tree.\n",
        "\n",
        "These start with a text that reads \"x[0] <=\" followed by a number.\n",
        "\n",
        "What do these numbers mean, and what do they have to do with the psuedo-posterior predictive mean (the blue line) from the scatter plot?"
      ],
      "metadata": {
        "id": "tzFG7Owtf5Jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task3**:\n",
        "\n",
        "There are two other numbers in each node: \"squared error\" and samples.\n",
        "\n",
        "What do those mean?"
      ],
      "metadata": {
        "id": "UB-Y5Vxwh1R_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task4**:\n",
        "\n",
        "Change max_depth to equal 5 in the above code, and the recreate both graphs.\n",
        "\n",
        "What changes?\n",
        "\n",
        "What did max_depth do?"
      ],
      "metadata": {
        "id": "t0Ch6RBqi3ET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task5**:\n",
        "\n",
        "Was increasing the depth of the tree a good idea for this data set?\n",
        "\n",
        "Why or why not? Try to use one of \"underfitting\" or \"overfitting\" in your answer."
      ],
      "metadata": {
        "id": "5OWtVX6Kjjym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task6:**\n",
        "\n",
        "Name some pros and cons to using a decision tree. Think about what they do better than other models you've seen, and what they do worse."
      ],
      "metadata": {
        "id": "5df3eyd9sj6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the answer key once you've completed the above tasks, and before moving on."
      ],
      "metadata": {
        "id": "YU4vqHN6_fFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forests (RF) and BART\n",
        "\n",
        "A randomized decision tree on it's own needs lots of care and direction to make sure it doesn't overfit--but a whole bunch of decision trees with randomized root nodes, added together, police themselves. Any method that combines the results of multiple randomized decision trees is called a Random Forest (RF). RFs turn out to be pretty accurate, and also one of the more interpretable model types, thanks to being built from decision trees. Increasing the number of trees can make RFs very accurate, and RFs can be exceptionally fast to fit if the number of trees isn't too big.\n",
        "\n",
        "Why is combining trees so much better? If we ensure that there's a diversity of trees (ensure they aren't all making similar decisions), then each tree is picking up on a different pattern in the data. Combining the results of the trees combines all the patterns each tree picked up, resulting in a model that \"knows\" about all of the patterns and can make better predictions. This is called ensemble learning, where a bunch of weak learners are combined to result in more accurate predictions.\n",
        "\n",
        "At least, that's what we hope happens. There's no guranttee that each tree will pick up something unique about the data. Luckily, we can ecourage our trees to find different patterns by using a BART model.\n",
        "\n",
        "Here's how: In addition to the randomized root nodes of a a basic RF, BART builds its trees in a sort of sequential manner. BART actually starts with all the trees it needs, but then randomly alters each tree based on the other trees. The \"based on the other trees\" bit ensures that BART is encouraging the growth of different trees. This idea is called boosting.\n",
        "\n",
        "BART goes further though; it selects each change to one of its trees as part of an MCMC. This ensures that the whole BART model is a sample from some posterior distribution of possible random forests. You can find a slightly more in depth summary of the BART model as described [here](https://www.youtube.com/watch?v=xWhPwHZF4c0).\n",
        "\n",
        "There's a final step also unique to BART, that the video above doesn't really cover: BART puts regularizing priors on the depth of each decision tree, and on the magnitude of the leaf nodes. The priors over the depth helps ensure that all the trees will be shallow, that the depth will be much less than the number of data points.\n",
        "Priors on the leaf nodes ensures that our model only explores near the actual data; the leaf node priors are more traditional regularizing priors.\n",
        "\n"
      ],
      "metadata": {
        "id": "lIQ8tVi0EeIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why all the background on how BART works?\n",
        "\n",
        "BART is a particularly good example of the algorithm buidling process. It started with something that works with many flaws (Decision Trees), imporved to something that works pretty well on its on (RFs), but kept adding things that helped either reduce underfitting (boosting) or reduce overfitting (MCMC, priors). The result is a flexible model that can accurately fit data *and* that we don't have to spend forever tuning.\n",
        "\n",
        "That push to make a better algorithm happened to create something that approximates a known mathematical object: a BART model with infinite trees is a nowhere differentiable Guassian Process. BART models are often much faster than any type of GP, and *way* easier to use. They are also simple to interpret, b/c their building blocks, decision trees, are easy to interpret.\n",
        "\n",
        "Additionally, BART-like models seem to perform much better than anything else at causal inference problems. I don't know if anyone is quite sure why yet (maybe it has something to do with the sequential nature of defining the trees, combined with the tree-graph sturcture itself??).\n",
        "\n",
        "One last time: people invented a good model by considering how to best balance underfitting, overfitting, speed, and interpretability."
      ],
      "metadata": {
        "id": "W0L-H_pdDarH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing BART in PyMC\n",
        "\n",
        "Armed with knowledge of how BART works and why we should use it, let's build a model with it."
      ],
      "metadata": {
        "id": "gEMTqno1R233"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Octupus BART"
      ],
      "metadata": {
        "id": "FOFGhg9NYxLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First is some preliz stuff I used to come up with a decent likelihood. You don't need to run it, I just thought I'd leave it here so y'all could see a bit of my thought process."
      ],
      "metadata": {
        "id": "qa2y9WPoyKtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import preliz as pz"
      ],
      "metadata": {
        "id": "tms_aVlhdL83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(octps.totWt)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Jbv45fftaqW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pz.maxent(pz.Gamma(), 100, 3000, 0.85)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CN0IMulUdOGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pz.Gamma(1,5).plot_pdf()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YWOoioPXhvuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A more thoughtful look at the Octopus Data\n",
        "\n",
        "Now that we're gonna make an actual model, let's take a proper tour of the data we're playig with."
      ],
      "metadata": {
        "id": "Rcj93BctNzvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We never looked at the actual dataframe, which was bad form. Let's do it now."
      ],
      "metadata": {
        "id": "wvvtSR6yyitx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "octps.head()"
      ],
      "metadata": {
        "id": "Mf1dFXbdZDXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should make a pair plot and causal diagram too."
      ],
      "metadata": {
        "id": "itf9ZmZBIpJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(octps)"
      ],
      "metadata": {
        "id": "CQ_iASvnPaoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "octodag = gv.Digraph(name=\"OctoDAG\")\n",
        "\n",
        "octodag.node('T','total weight')\n",
        "octodag.node('U','upper beak length')\n",
        "octodag.node('L','lower beak length')\n",
        "octodag.node('W','lateral wall length')\n",
        "\n",
        "octodag.edge('U','L', dir = 'both')\n",
        "octodag.edge('L','W', dir = 'both')\n",
        "octodag.edge('U','W', dir = 'both')\n",
        "octodag.edges(['UT','LT','WT',])\n",
        "\n",
        "octodag"
      ],
      "metadata": {
        "id": "ugwFqAzxPesZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I don't know enough about octopuses, beaks, or octopus beaks to try to even guess at the causal direction between upper, lower and lateral wall. They definitely interact though, as seen from the pair plot.\n",
        "\n",
        "I'm more comfortable assuming that the beak length measurements cause weigth--this is like saying height causes weight in a person."
      ],
      "metadata": {
        "id": "Fh-4PaCoNXY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code is just here to remind you what X and Y are. It also has features commented out; that can be a nice way to put multiple estimators/predictiors into a BART model."
      ],
      "metadata": {
        "id": "1kDv0A-szCpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#features = [\"upBeak\", \"loBeak\", \"latWall\"]\n",
        "\n",
        "X = octps[\"upBeak\"].to_numpy().reshape(-1, 1)\n",
        "Y = octps[\"totWt\"].to_numpy()"
      ],
      "metadata": {
        "id": "4tBAeUWJY-oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Making our first octopus beak model"
      ],
      "metadata": {
        "id": "D3NigCliOiiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task7**:\n",
        "\n",
        "Examine the model below. Is the BART part of the model a prior, or a likelihood?\n",
        "\n",
        "Were GPs priors, or likelihoods?"
      ],
      "metadata": {
        "id": "z-nAExz9zWmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with pm.Model() as model_octps:\n",
        "    s = pm.HalfNormal('s',1000)\n",
        "    μ_ = pmb.BART(\"μ_\", X, np.log(Y), m=50)\n",
        "    μ = pm.Deterministic(\"μ\",pm.math.exp(μ_))\n",
        "    y = pm.Gamma(\"y\", mu=μ, sigma=s,  observed=Y)\n",
        "    idata_octps = pm.sample(compute_convergence_checks=False)"
      ],
      "metadata": {
        "id": "ObO9bNQRZSmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate the posterior predictive dist\n",
        "pm.sample_posterior_predictive(idata_octps,model_octps, extend_inferencedata=True)\n",
        "az.plot_ppc(idata_octps, num_pp_samples=100, colors=[\"C1\", \"C0\", \"C1\"])"
      ],
      "metadata": {
        "id": "tVpHj5WTnJqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posterior_mean = idata_octps.posterior[\"μ\"]\n",
        "\n",
        "μ_hdi = az.hdi(ary=idata_octps, group=\"posterior\", var_names=[\"μ\"], hdi_prob=0.74)\n",
        "\n",
        "pps = az.extract(\n",
        "    idata_octps, group=\"posterior_predictive\", var_names=[\"y\"]\n",
        ").T"
      ],
      "metadata": {
        "id": "t3zi1I9ay1TX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.argsort(X[:, 0])\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "az.plot_hdi(\n",
        "    x=X[:, 0],\n",
        "    y=pps,\n",
        "    ax=ax,\n",
        "    hdi_prob=0.93,\n",
        "    fill_kwargs={\"alpha\": 0.3, \"label\": r\"Posterior Predictive $93\\%$ HDI\"},\n",
        ")\n",
        "\n",
        "az.plot_hdi(\n",
        "    x=X[:, 0],\n",
        "    y=posterior_mean,\n",
        "    ax=ax,\n",
        "    hdi_prob=0.74,\n",
        "    fill_kwargs={\"alpha\": 0.6, \"label\": r\"Mean $74\\%$ HDI\"},\n",
        ")\n",
        "ax.plot(octps[\"upBeak\"], octps[\"totWt\"], \"o\", c=\"C0\", label=\"Raw Data\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "ax.set(\n",
        "    title=\"Posterior Predictive\",\n",
        "    xlabel=\"upBeak\",\n",
        "    ylabel=\"totWt\",\n",
        ");"
      ],
      "metadata": {
        "id": "5Avcfnqx3MzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE**: Because BART models--and random forests in general--are just a bunch of horzontal and vertical lines taped together, they have a bias towards \"flatness\" (or parallelness to the predictor) on the left and right edges of any predictive plot."
      ],
      "metadata": {
        "id": "09iQPYGv_oKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task8**:\n",
        "\n",
        "Comment on the fit of the above posterior predictive check. Is it good, bad, or ugly?"
      ],
      "metadata": {
        "id": "XS3awOgu3w1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Counties Data, PDPs, ICE, and VI plots\n",
        "\n",
        "Trigger warning: This analysis covers suicide, and it attempts to find things that influence suicide rates by county in the USA. If you need support in dealing with this topic, reach out. Your voice matters\n",
        "\n",
        "You can find a full explanation of the counties dataset [here](https://github.com/evangambit/JsonOfCounties?tab=readme-ov-file). The person who curated and intially cleaned the data (Evan Gambit?), did great work, and made this analysis possible. I do not know them, but I am grateful.\n",
        "\n",
        "After some exploratory data analysis, we'll learn about some new tools unique to BART.\n",
        "\n",
        "pdps (partial dependence plots) will allow us to see how each predictor variable relates to suicide rate--assuming there are no interactions between predictor variables.\n",
        "\n",
        "ice (individual condtional expectation) plots will allows to see if their are interactions, and even give us a hint as to the nature of the interactions.\n",
        "\n",
        "vi (variable importantce) plots will give us an idea of which variables contribute more predictive power.\n",
        "\n",
        "Altogether, the plots will give us a clearer picture of what our model is, what it values, and what relationships exist in the data."
      ],
      "metadata": {
        "id": "F2Iu-Sh7OrcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data and Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "2IGgHty-QQ7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First though, is a bit of data cleaning. Not all the columns seem relevant to predict suicide; I arbitrarily chose subset of variables\n",
        "\n",
        "From those, I drop even more to get to my causal diagram. And my actual models have even fewer than in the causal diagram.\n",
        "\n",
        "Remember, all models are abritrary, and someone could easily decide a different set of variables are relevant, and end up with a different analysis than me."
      ],
      "metadata": {
        "id": "JUdanwtNyuyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counties = pd.read_csv('https://raw.githubusercontent.com/thedarredondo/data-science-fundamentals/refs/heads/main/Data/counties.csv')"
      ],
      "metadata": {
        "id": "JgctW_UZ_zDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the following line will let you see all the variables/columns\n",
        "#counties.columns.values"
      ],
      "metadata": {
        "id": "aOzR8mwdsLRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clmn_list = ['name',\n",
        "             'state',\n",
        "             'population/2019',\n",
        "             'health/Violent Crime Rate',\n",
        "             'life-expectancy',\n",
        "             'health/Teen Birth Rate',\n",
        "             'poverty-rate',\n",
        "             'health/Average Number of Mentally Unhealthy Days',\n",
        "             'health/% Excessive Drinking',\n",
        "             'edu/bachelors+',\n",
        "             'avg_income',\n",
        "             'deaths/suicides',\n",
        "             'edu/less-than-high-school',\n",
        "             'elections/2020/gop',\n",
        "             ]\n",
        "sad_stats =  counties.loc[:,clmn_list]"
      ],
      "metadata": {
        "id": "vphc4l6WBxcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also drop any rows that don't have data for any one of the variables. This removed around 300 to 400 hundred counties. Perhaps having data for those counties would alter the ensuing results?\n",
        "\n",
        "I won't attempt to answer that question for two reasons:\n",
        "- creating a model for the missing data would require me to learn about every county that we dropped, and/or learning about the data collection method, which I don't have time for.\n",
        "- We have a large enough set of data that we can still learn something about the world."
      ],
      "metadata": {
        "id": "MP03Exj2hMTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdsts_nona = sad_stats.dropna()"
      ],
      "metadata": {
        "id": "MVF4Y8NxHz0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I next weight the raw counts by county population. This is not representative of proportion of population (its unclear how deaths by suicide influnce the offical poulation count, for example), but it does allow me to compare large counties to small counties. Unlike with raw counts."
      ],
      "metadata": {
        "id": "9RMTZab4jh4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdsts_nona.insert(sdsts_nona.shape[1], \"suicide/pop\", sdsts_nona['deaths/suicides']/sdsts_nona['population/2019'])\n",
        "sdsts_nona.insert(sdsts_nona.shape[1], \"gunsuicide/pop\", sdsts_nona['deaths/firearm suicides']/sdsts_nona['population/2019'])\n",
        "sdsts_nona.insert(sdsts_nona.shape[1], \"<hs/pop\", sdsts_nona['edu/less-than-high-school']/sdsts_nona['population/2019'])\n",
        "sdsts_nona.insert(sdsts_nona.shape[1], \"gop/pop\", sdsts_nona['elections/2020/gop']/sdsts_nona['population/2019'])\n",
        "sdsts_nona.insert(sdsts_nona.shape[1], \"bachelors+/pop\", sdsts_nona['edu/bachelors+']/sdsts_nona['population/2019'])\n",
        "sdsts_nona.insert(sdsts_nona.shape[1], \"nogunsuicide/pop\", sdsts_nona['suicide/pop'] - sdsts_nona['gunsuicide/pop'])\n",
        "sdsts_nona.drop(['deaths/suicides', 'edu/less-than-high-school','elections/2020/gop','edu/bachelors+','deaths/firearm suicides',], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "tKAHga3yW4dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this column name is too dang long, so I shorten it\n",
        "sdsts_nona.rename(columns={'health/Average Number of Mentally Unhealthy Days':'mentUnhealth','health/Teen Birth Rate':'teenBirth'}, inplace=True)"
      ],
      "metadata": {
        "id": "y2trb1W0SWO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's what the final dataset looks like:"
      ],
      "metadata": {
        "id": "2ZO2wwYoryvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdsts_nona.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9DUvlFpIzAhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(sdsts_nona)"
      ],
      "metadata": {
        "id": "K7jTPM3-VuE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Causal diagram time--but to make it easier on myself, I'm only going to include: gun suicide rate, no gun suicide rate, teen birth rate, poverty rate, number of mentally unhealthy days, proportion with less than a highschool education, and proportion who voted for the GOP in the 2020 electio.\n",
        "\n",
        "Note that no gun suicide rate is literally suicide rate subtracted by firearm suicide rate.\n"
      ],
      "metadata": {
        "id": "FwmjnvRzIMGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#c for counties\n",
        "cdag = gv.Digraph(name=\"CountiesDAG\")\n",
        "\n",
        "cdag.node('S','Suicide rate')\n",
        "cdag.node('F','Firearm suicide rate')\n",
        "cdag.node('N','No gun suicide rate')\n",
        "cdag.node('T','Teen birth rate')\n",
        "cdag.node('P','Poverty rate')\n",
        "cdag.node('M','Mentally unhealthy days')\n",
        "cdag.node('H','less than High school')\n",
        "cdag.node('G','GOP vote')\n",
        "\n",
        "cdag.edge('F','S', dir = 'both')\n",
        "cdag.edge('T','H', dir = 'both')\n",
        "#cdag.edge('U','W', dir = 'both')\n",
        "cdag.edges([\n",
        "    'SN','FN','TS','MS','PS','HS','TF','MF','PF','HF','GF','PM','PT','PH','MH','MT',\n",
        "    ])\n",
        "\n",
        "cdag"
      ],
      "metadata": {
        "id": "OGcezm-NIRcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remember, the above graph is a visualization of MY *assumptions*. It does not represent fact.**"
      ],
      "metadata": {
        "id": "FEy4cx5WpFLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for some spoilers--shout out to former student Alex Hersh for discovering this in the eleventh hour of his Unit 8 project; I didn't even have the firearm suicide variable in my original analysis.\n",
        "\n",
        "Note in the above causal diagram that I only have GOP vote causing firearm suicide rate, not suicide rate, despite the fact that suicide rate and GOP vote appear correlated in the pair plot. I assume that people who vote GOP have more guns than none GOP voters, and I'm noticing that there is no correlation between no gun suicide rate and GOP.\n",
        "\n",
        "These points might be made clearer by examining the plots below."
      ],
      "metadata": {
        "id": "F3o9QYLszSTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data = sdsts_nona, y = 'suicide/pop', x = 'gop/pop', hue = 'poverty-rate')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "z0J7aCzOzUkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data = sdsts_nona, y = 'nogunsuicide/pop', x = 'gop/pop', hue = 'poverty-rate',)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9HawRcx7zXfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prep Data for BART Model"
      ],
      "metadata": {
        "id": "EIbkkQ1TQZx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the exploratory data analysis and causal diagramming, I only used teen birth rate, poverty rate, number of mentally unhealthy days, proportion with less than a highschool education, and proportion who voted for the GOP in the 2020 election to predict suicide rate.\n",
        "\n",
        "I am purposely including GOP in here, even though my causal diagram says not to. This is to:\n",
        "\n",
        "1. Show what can happen when I make a poor causal assumption\n",
        "2. Own that I may this mistake on my first pass with this data; I didn't make a causal diagram, and thus missed a key insight."
      ],
      "metadata": {
        "id": "AqdT427Yzg5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine all my estimator variables into one dataframe\n",
        "X = sdsts_nona[[\n",
        "             'teenBirth',\n",
        "             'poverty-rate',\n",
        "             'mentUnhealth',\n",
        "             '<hs/pop',\n",
        "             'gop/pop',\n",
        "             ]]\n",
        "#Define my estimand as Y\n",
        "Y = sdsts_nona['suicide/pop']"
      ],
      "metadata": {
        "id": "7YU7Dnjwzlg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BART Model (predicting suicide rate)"
      ],
      "metadata": {
        "id": "9si3uZyHzohq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the model, in a svelte six lines of code.\n",
        "\n",
        "Suicide rate is again all positive (like octupus beak weight), so I use a gamma likelihood. There are also the same log and exp shenanigans from before; these serve to make suicide rate go into the negatives, b/c BART is a real numbers 4eva type of gal.\n",
        "\n",
        "But seriously, the pymc implementation of BART needs to have its predicted range over positive and negative values, so we have to do this for any all positive support likelihood. I probably could have gotten away with using a normal likelihood--the histogram for suicides isn't that skewed--but I wanted to do my best."
      ],
      "metadata": {
        "id": "pXSWN2vMzrxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with pm.Model() as model_counties:\n",
        "    s = pm.HalfNormal('s',20) #sigma = 20 is a blind guess\n",
        "    μ_ = pmb.BART(\"μ_\", X, np.log(Y), m=50) #log around Y\n",
        "    μ = pm.Deterministic(\"μ\",pm.math.exp(μ_)) #exp it all once BART is done\n",
        "    y = pm.Gamma(\"y\", mu=μ, sigma=s,  observed=Y) #likelihood\n",
        "    idata_counties = pm.sample(compute_convergence_checks=False)"
      ],
      "metadata": {
        "id": "dEj2p2LUT56p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pm.sample_posterior_predictive(idata_counties,model_counties, extend_inferencedata=True)"
      ],
      "metadata": {
        "id": "D7mqir1AUIaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of y'all may look at this and think \"inverse gamma would definitely be better\" but every time I tried that likelihood, if gave me strange behavior. Maybe you'll have better luck if you try."
      ],
      "metadata": {
        "id": "0eUah6BlnAZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = az.plot_ppc(idata_counties, num_pp_samples=100, colors=[\"C1\", \"C0\", \"C1\"])\n",
        "ax.set_xlim(0.0,0.0005)"
      ],
      "metadata": {
        "id": "eebD_7u6dMrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next two code blocks let you plot the posterior predictive for a single one of the variables versus suicide rate. This is less useful, in my opinion, than what follows."
      ],
      "metadata": {
        "id": "DmVNFokfniq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "posterior_mean = idata_counties.posterior[\"μ\"]\n",
        "\n",
        "μ_hdi = az.hdi(ary=idata_counties, group=\"posterior\", var_names=[\"μ\"], hdi_prob=0.74)\n",
        "\n",
        "pps = az.extract(\n",
        "    idata_counties, group=\"posterior_predictive\", var_names=[\"y\"]\n",
        ").T"
      ],
      "metadata": {
        "id": "a_jnIgSvafae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.argsort(X.to_numpy()[:, 4]) #grab fifth row\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "az.plot_hdi(\n",
        "    x=X.to_numpy()[:, 4],#grab fifth row\n",
        "    y=pps,\n",
        "    ax=ax,\n",
        "    hdi_prob=0.93,\n",
        "    fill_kwargs={\"alpha\": 0.3, \"label\": r\"Posterior predictive $93\\%$ HDI\"},\n",
        ")\n",
        "\n",
        "az.plot_hdi(\n",
        "    x=X.to_numpy()[:, 4],#grab fifth row\n",
        "    y=posterior_mean,\n",
        "    ax=ax,\n",
        "    hdi_prob=0.74,\n",
        "    fill_kwargs={\"alpha\": 0.6, \"label\": r\"Mean $74\\%$ HDI\"},\n",
        ")\n",
        "ax.plot(sdsts_nona['gop/pop'], sdsts_nona[\"suicide/pop\"], \"o\", c=\"C0\", label=\"Raw Data\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "ax.set(\n",
        "    title=\"Posterior Predictive\",\n",
        "    xlabel=\"gop/pop\",\n",
        "    ylabel=\"suicide/pop\",\n",
        ");"
      ],
      "metadata": {
        "id": "MLF8z4oScaIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partial Dependence Plots (pdp)\n",
        "\n",
        "This is the beginning of the exciting stuff.\n",
        "\n",
        "The five graphs below show the association between a predictor varible and the predicted variable, marginalized over the other variables.\n",
        "\n",
        "Why is that exciting?\n",
        "\n",
        "It means that these plots show the affect of the proportion who voted for the GOP in 2020 conditioning on the effect of poverty, teen birth rate, reported mental health, and under-education...\n",
        "\n",
        "...**ASSUMING** that all those predictor variables are mostly independent."
      ],
      "metadata": {
        "id": "-8H0eJSXn6ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pmb.plot_pdp(μ_, X, Y, grid=(1, 5), func=np.exp, figsize = (12,6))"
      ],
      "metadata": {
        "id": "WjobGRSEWwxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task9**:\n",
        "\n",
        "Interpret the gop/pop versus suicide rate partial dependence plot, assuming approximate independence between the predictor variables.\n",
        "\n",
        " What is the trend?\n",
        "\n",
        " Why?"
      ],
      "metadata": {
        "id": "tBqfGJtqpibo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Individual Conditional Expectation (ice) Plots\n",
        "\n",
        "Let's drop the assumption that each of the predictor variables is predictor variables is indepentent--b/c they aren't.\n",
        "\n",
        "For example, education level and GOP voting tendency have had a pretty stable negative association in last couple of decades. And poverty, teen birth rate, education, and mental health are all likely interacting with each other as well.\n",
        "\n",
        "The first step to account for that is an ice plot.\n",
        "\n",
        "Look at the lines that make up the ice plots below. The average of all those lines makes up the pdp from above; each line n the ice plot represents a single observation. All those lines averaged are teh pdp.\n",
        "\n",
        "Individual predictions per data point help us see trends that aren't present in the average.\n",
        "\n",
        "What do we look for? Parallel-ness: if all the lines in each plot are parallel, then when can ignore the ice plot, and go back to the pdp"
      ],
      "metadata": {
        "id": "wpwL_AYCvYfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pmb.plot_ice( μ_, X, Y, grid=(1, 5), func=np.exp, figsize = (12,6))"
      ],
      "metadata": {
        "id": "d6wlj9RzXo5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 10**:\n",
        "\n",
        "Are the lines within each plot parallel, in the ice plots above?\n",
        "\n",
        "If they aren't parallel, then describe and interpret the areas in which the lines within the plots are not parallel.\n",
        "\n",
        "Hint: The lines aren't parallel. Get to describing and interpreting."
      ],
      "metadata": {
        "id": "EdcdpLE9xSed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variable Importance (vi) Plots\n",
        "\n",
        "VI plots, in theory, are supposed to tell us which variables to select if all we care about is prediction accruacy and speed.\n",
        "\n",
        "For example, the plot below seems to indicate we could probably drop teen birth rate and number of bad mental health days, and get a similar level of performance. It also seems to indicate that gop tendancy and poverty rate give the biggest boosts in performance, as there jumps up in $R^{2}$ are largest.\n",
        "\n",
        "There are lots of caveats to those interpretations though.\n",
        "\n",
        "- The order in which the model calculated the $R^{2}$ values matters--and the order is not staable (i.e. it's different from run to run)\n",
        "- we're using $R^{2}$ instead of elpd_loo, b/c pymc hasn't implemented elpd_loo for BART.\n",
        "- The order in which BART calculates $R^{2}$ matters a little, and unfortuantely I don't know the scheme for the order it chooses.\n",
        "- this plot does not show us variable interactions. BART is almost certainly applying the equivalent of variable interactions, but our only hint about what those interactions are comes from the ice plots above"
      ],
      "metadata": {
        "id": "V3UBDZH-1aNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vi_counties = pmb.compute_variable_importance(idata_counties, μ_, X)"
      ],
      "metadata": {
        "id": "gC68I6Z0CvJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pmb.plot_variable_importance(vi_counties)"
      ],
      "metadata": {
        "id": "K7krnmh9XNvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion to predicting sucicide rate by county using BART\n",
        "\n"
      ],
      "metadata": {
        "id": "IER7owki42Bm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task11**:\n",
        "\n",
        "Summarize everything we learned from and about model_counties."
      ],
      "metadata": {
        "id": "VpXStAtA7Tkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to read the answer key before moving on to the next section."
      ],
      "metadata": {
        "id": "eCmztOqy0RSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BART (predicting no gun suicide rate)\n",
        "\n",
        "Let's do that again, but this time predict no gun suicide rate, and obey my causal diagram."
      ],
      "metadata": {
        "id": "gAEsYJ0TV_gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine all my estimator variables into one dataframe\n",
        "X = sdsts_nona[[\n",
        "             'teenBirth',\n",
        "             'poverty-rate',\n",
        "             'mentUnhealth',\n",
        "             '<hs/pop',\n",
        "             'gop/pop',\n",
        "             ]]\n",
        "#Define my estimand as Y, normalized\n",
        "Y = (sdsts_nona['nogunsuicide/pop'] - sdsts_nona['nogunsuicide/pop'].mean())/sdsts_nona['nogunsuicide/pop'].std()"
      ],
      "metadata": {
        "id": "F94dtcpHWFto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with pm.Model() as model_nogun:\n",
        "    s = pm.HalfNormal('s',10)\n",
        "    #I normalized the Y values, so no need for log and exp shenanigans\n",
        "    μ_ = pmb.BART(\"μ_\", X, Y, m=50)\n",
        "    μ = pm.Deterministic(\"μ\",μ_)\n",
        "    #it's not actually normal, but I just want a rough sketch\n",
        "    y = pm.Normal(\"y\", mu=μ, sigma=s,  observed=Y)\n",
        "    idata_nogun = pm.sample(compute_convergence_checks=False)"
      ],
      "metadata": {
        "id": "qGU4CAD8WuXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pm.sample_posterior_predictive(idata_nogun,model_nogun, extend_inferencedata=True)"
      ],
      "metadata": {
        "id": "BpytFuSBXK2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = az.plot_ppc(idata_nogun, num_pp_samples=100, colors=[\"C1\", \"C0\", \"C1\"])\n",
        "#ax.set_xlim(0.0,0.0005)"
      ],
      "metadata": {
        "id": "xBcSqv-YZfr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posterior_mean = idata_nogun.posterior[\"μ\"]\n",
        "\n",
        "μ_hdi = az.hdi(ary=idata_nogun, group=\"posterior\", var_names=[\"μ\"], hdi_prob=0.74)\n",
        "\n",
        "pps = az.extract(\n",
        "    idata_nogun, group=\"posterior_predictive\", var_names=[\"y\"]\n",
        ").T"
      ],
      "metadata": {
        "id": "WYCKjYJGZ19v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.argsort(X.to_numpy()[:, 4]) #grab fifth row\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "az.plot_hdi(\n",
        "    x=X.to_numpy()[:, 4],#grab fifth row\n",
        "    y=pps,\n",
        "    ax=ax,\n",
        "    hdi_prob=0.93,\n",
        "    fill_kwargs={\"alpha\": 0.3, \"label\": r\"Posterior predictive $93\\%$ HDI\"},\n",
        ")\n",
        "\n",
        "az.plot_hdi(\n",
        "    x=X.to_numpy()[:, 4],#grab fifth row\n",
        "    y=posterior_mean,\n",
        "    ax=ax,\n",
        "    hdi_prob=0.74,\n",
        "    fill_kwargs={\"alpha\": 0.6, \"label\": r\"Mean $74\\%$ HDI\"},\n",
        ")\n",
        "ax.plot(sdsts_nona['gop/pop'], Y, \"o\", c=\"C0\", label=\"Raw Data\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "ax.set(\n",
        "    title=\"Posterior Predictive\",\n",
        "    xlabel=\"gop/pop\",\n",
        "    ylabel=\"nogunsuicide/pop\",\n",
        ");"
      ],
      "metadata": {
        "id": "yjl6vWQRaDkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pmb.plot_pdp(μ_, X, Y, grid=(1, 5), func=np.exp, figsize = (12,6))"
      ],
      "metadata": {
        "id": "90eywj5-bw6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pmb.plot_ice( μ_, X, Y, grid=(1, 5), func=np.exp, figsize = (12,6))"
      ],
      "metadata": {
        "id": "XZpvCTwub-YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vi_nogun = pmb.compute_variable_importance(idata_nogun, μ_, X)"
      ],
      "metadata": {
        "id": "fkeo0M69fGqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pmb.plot_variable_importance(vi_nogun)"
      ],
      "metadata": {
        "id": "LrTWY5XQfN4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task12:**\n",
        "\n",
        "Summarize everything we learned from and about model_nogun."
      ],
      "metadata": {
        "id": "FimwMCqvcBGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BART (predicting firearm suicides)\n",
        "\n",
        "That's right, round three.\n",
        "\n",
        "Seperating firearm and nonfirearm suicides makes sense to me, since firearms are designed to kill people [citation needed]."
      ],
      "metadata": {
        "id": "DaPmomVOgE3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm going to normalize things again, because I'm lazy, and I just want to see the big picture. Ignoring the skew here is a little more egregious (see the histogram in the pair plot); let's see if I get punished for my sloth."
      ],
      "metadata": {
        "id": "vvmVM8G0hYNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine all my estimator variables into one dataframe\n",
        "X = sdsts_nona[[\n",
        "             'teenBirth',\n",
        "             'poverty-rate',\n",
        "             'mentUnhealth',\n",
        "             '<hs/pop',\n",
        "             'gop/pop',\n",
        "             ]]\n",
        "#Define my estimand as Y, normalized\n",
        "Y = (sdsts_nona['gunsuicide/pop'] - sdsts_nona['gunsuicide/pop'].mean())/sdsts_nona['gunsuicide/pop'].std()"
      ],
      "metadata": {
        "id": "DRzfhwWrhHG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with pm.Model() as model_gun:\n",
        "    s = pm.HalfNormal('s',10)\n",
        "    #I normalized the Y values, so no need for log and exp shenanigans\n",
        "    μ_ = pmb.BART(\"μ_\", X, Y, m=50)\n",
        "    μ = pm.Deterministic(\"μ\",μ_)\n",
        "    #it's not actually normal, but I just want a rough sketch\n",
        "    y = pm.Normal(\"y\", mu=μ, sigma=s,  observed=Y)\n",
        "    idata_gun = pm.sample(compute_convergence_checks=False)"
      ],
      "metadata": {
        "id": "BK7f05BrhkdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pm.sample_posterior_predictive(idata_gun,model_gun, extend_inferencedata=True)"
      ],
      "metadata": {
        "id": "6Amf6qMYh8jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = az.plot_ppc(idata_gun, num_pp_samples=100, colors=[\"C1\", \"C0\", \"C1\"])\n",
        "#ax.set_xlim(0.0,0.0005)"
      ],
      "metadata": {
        "id": "-BMN3mHSiD6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definitely off on the likelihood, but its good enough for a rough sketch."
      ],
      "metadata": {
        "id": "ka2BY5r2ii2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "posterior_mean = idata_gun.posterior[\"μ\"]\n",
        "\n",
        "μ_hdi = az.hdi(ary=idata_gun, group=\"posterior\", var_names=[\"μ\"], hdi_prob=0.74)\n",
        "\n",
        "pps = az.extract(\n",
        "    idata_nogun, group=\"posterior_predictive\", var_names=[\"y\"]\n",
        ").T"
      ],
      "metadata": {
        "id": "mTHto4j6iK1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.argsort(X.to_numpy()[:, 4]) #grab fifth row\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "az.plot_hdi(\n",
        "    x=X.to_numpy()[:, 4],#grab fifth row\n",
        "    y=pps,\n",
        "    ax=ax,\n",
        "    hdi_prob=0.93,\n",
        "    fill_kwargs={\"alpha\": 0.3, \"label\": r\"Posterior predictive $93\\%$ HDI\"},\n",
        ")\n",
        "\n",
        "az.plot_hdi(\n",
        "    x=X.to_numpy()[:, 4],#grab fifth row\n",
        "    y=posterior_mean,\n",
        "    ax=ax,\n",
        "    hdi_prob=0.74,\n",
        "    fill_kwargs={\"alpha\": 0.6, \"label\": r\"Mean $74\\%$ HDI\"},\n",
        ")\n",
        "ax.plot(sdsts_nona['gop/pop'], Y, \"o\", c=\"C0\", label=\"Raw Data\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "ax.set(\n",
        "    title=\"Posterior Predictive\",\n",
        "    xlabel=\"gop/pop\",\n",
        "    ylabel=\"gunsuicide/pop\",\n",
        ");"
      ],
      "metadata": {
        "id": "z3GOU67LiR-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pmb.plot_pdp(μ_, X, Y, grid=(1, 5), func=np.exp, figsize = (12,6))"
      ],
      "metadata": {
        "id": "1chukQ1liyDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pmb.plot_ice( μ_, X, Y, grid=(1, 5), func=np.exp, figsize = (12,6))"
      ],
      "metadata": {
        "id": "1MkAf--Gi4RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vi_gun = pmb.compute_variable_importance(idata_gun, μ_, X)"
      ],
      "metadata": {
        "id": "M7Xf_bBli932"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pmb.plot_variable_importance(vi_gun)"
      ],
      "metadata": {
        "id": "BIbgqvitjA7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task13:**\n",
        "\n",
        "Summarize everything we learned from and about model_gun."
      ],
      "metadata": {
        "id": "srz8tIHpj-8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "Once again, we've covered a lot of ground.\n",
        "\n",
        "We put in the work to start understanding the concept of decision trees, how they can flexibly adapt to data, in an interpretable way.\n",
        "\n",
        "Then we jumped into a theoretical disscussion on how to supe up our decions trees, making them more powerful.\n",
        "\n",
        "Those fancy, aggregated decision trees were called BART, and they allowed us to quickly create a model which could determine an appropriate non-linear shape, all on its own.\n",
        "\n",
        "Next, we pivoted to a different application of BART, with multiple predictiors. It still helped to have BART be able to capture non-linear trends, but we also used pdps, ice plots, and vi plots to find out the relationships between several predictor variables and suicide rate by county.\n",
        "\n",
        "We were able to determine some interesting associations, and also learn that there were many interactions between our predictor variables.\n",
        "\n",
        "Throughout, we made continual reference to the context of the data and our causal assumptions--no matter haow effective our tools, we still have to think about reality.\n",
        "\n",
        "BART is a powerful, flexible, and interpretable model. We can use the framework to quickly create a good prediction engine for simple to complex data. We can also use it to learn about relationships in our data."
      ],
      "metadata": {
        "id": "NOIJIYF02MqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What's next?\n",
        "\n",
        "So, is there anything BART (or random forests) can't do?\n",
        "\n",
        "BART and RFs can do almost anything a GP can do, which is a lot. But purer GPs are better for time series data, GLMs are better for precisely determing interactions and the nuances of data.\n",
        "\n",
        "And in the next unit, we'll cover a model type that trades in interpretability for (potentially) the highest predictive power of any model type.\n",
        "\n",
        "But neural networks are next unit. For now, we still value the interpretability of our model about as much (mayber even a little more) than its predictive power."
      ],
      "metadata": {
        "id": "6DMhEQABDDN-"
      }
    }
  ]
}